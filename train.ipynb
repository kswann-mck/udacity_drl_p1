{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Navigation\n",
    "## Udacity Deep Reinforcement Learning Nano-Degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to use the Deep Q-Learning algorithm to train an agent to solve the Unity Banana environment. The Unity Banana environment is a simple 3d environment consisting of a bounding box, and a number of blue and yellow bananas. A successful agent moves around collecting only the yellow bananas avoiding the blue ones.\n",
    "\n",
    "For the purpose of this assignment, the task is considered solved when the agent can get an average score of +13 over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Banana Environment\n",
    "\n",
    "### Actions\n",
    "\n",
    "At any time, the agent can take one of four actions:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "### States\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.\n",
    "\n",
    "### Rewards\n",
    "\n",
    "A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the basic structure of the neural network, and the dqn agent, I used the code provided in the [solution to the earlier exercise](https://github.com/udacity/deep-reinforcement-learning/tree/master/dqn/solution).\n",
    "\n",
    "I modified the training routine to work with the Unity environment's API format, and I modified the parameters of the training function, dgn agent, and neural network so I could conduct a hyperparmeter search using scikit-optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Size: 4, Action Size: 37\n"
     ]
    }
   ],
   "source": [
    "from train import dqn\n",
    "from optimize import find_optimal_hyperparameters\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import json\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from skopt.plots import plot_convergence\n",
    "\n",
    "# initialize the environment and brain\n",
    "env = UnityEnvironment(file_name=\"env/Banana.app\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# Get the action and state size from the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "ACTION_SIZE = brain.vector_action_space_size\n",
    "state = env_info.vector_observations[0]\n",
    "STATE_SIZE = len(state)\n",
    "\n",
    "print(f\"State Size: {ACTION_SIZE}, Action Size: {STATE_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(scores):\n",
    "    ## Plot the scoring results\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Point\n",
    "\n",
    "As a benchmark, I did a full training run using the parameters set in the initial implementation above. The results are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dqn(env=env, brain_name=brain_name, name=\"_base_model\", n_episodes=1800, break_early=True)\n",
    "\n",
    "plot_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Optimal Hyperparameters\n",
    "\n",
    "Because each training run is expensive, something like a grid search that isn't \"smart\" is not an ideal way to search for hyperparamters. I instead used the `scikit-optimize` package which implements bayesian hyperparameter optimization using gaussian process regression.\n",
    "\n",
    "In the file [optimize.py](optimize.py) I defined a paramater search over a number of parameters and defined a function to find optimal hyperparameters.\n",
    "\n",
    "To save time, I limited the number of episodes to 200 for each attempt during the hyperparameter search. My thinking here is that while the task won't be solved in 200 episodes, that should be enough to give a signal in terms of the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected `n_calls` >= 10, got 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c2398ab09d95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimal_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_gp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_optimal_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot_convergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_gp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/udacity_drl/udacity_drl_p1/optimize.py\u001b[0m in \u001b[0;36mfind_optimal_hyperparameters\u001b[0;34m(env, brain_name)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;34m\"\"\"Find and return optimal hyperparameter values for the agent.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mres_gp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgp_minimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# show the best score achieved with optimal hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/udacity_drl_p1/lib/python3.6/site-packages/skopt/optimizer/gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs, model_queue_size)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         callback=callback, n_jobs=n_jobs, model_queue_size=model_queue_size)\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/udacity_drl_p1/lib/python3.6/site-packages/skopt/optimizer/base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs, model_queue_size)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_calls\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mrequired_calls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         raise ValueError(\n\u001b[0;32m--> 249\u001b[0;31m             \"Expected `n_calls` >= %d, got %d\" % (required_calls, n_calls))\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calculate the total number of initial points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0mn_initial_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_initial_points\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected `n_calls` >= 10, got 2"
     ]
    }
   ],
   "source": [
    "optimal_params, res_gp, = find_optimal_hyperparameters(env=env, brain_name=brain_name)\n",
    "plot_convergence(res_gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Training Run with Optimized Parameters\n",
    "\n",
    "After finding optimal parameters using the search above, I conducted a full training run using the new parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_params = {\n",
    "  \"tau\": 3.8801065755411095e-05,\n",
    "  \"gamma\": 1.0,\n",
    "  \"lr\": 5.288386055674643e-05,\n",
    "  \"fc2_units\": 115,\n",
    "  \"fc1_units\": 111,\n",
    "  \"update_every\": 2,\n",
    "  \"batch_size\": 57,\n",
    "  \"eps_decay\": 0.00022572684710474913,\n",
    "  \"eps_end\": 0.01,\n",
    "  \"eps_start\": 1.0,\n",
    "  \"max_t\": 1000,\n",
    "  \"solved_threshold\": 13.0,\n",
    "  \"action_size\": 4,\n",
    "  \"state_size\": 37\n",
    "}\n",
    "\n",
    "\n",
    "scores = dqn(\n",
    "    env=env, \n",
    "    brain_name=brain_name, \n",
    "    name=\"_optimal_model\", \n",
    "    n_episodes=1800, \n",
    "    break_early=True,\n",
    "    **optimal_params\n",
    ")\n",
    "\n",
    "plot_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, after optimizing the hyperparameters, the agent solves this task in only 240 episodes compared with 496 episodes, which is a pretty significant improvement. However, looking at the score plot the results appear less stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watched the trained agent play!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
